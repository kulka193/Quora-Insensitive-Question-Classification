{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport os\nimport sys\nimport pandas as pd\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f70d9890cc044ed355b5521f1bb470a017f3df94"},"cell_type":"code","source":"from keras.preprocessing.sequence import utils,pad_sequences\nfrom keras.layers import Embedding,Conv2D,Flatten,BatchNormalization,LSTM,Dropout\nfrom keras.models import Sequential,Input,Model\nfrom keras.preprocessing.text import Tokenizer\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45c0b3b41b4a277915db92d9d9fbebca63770eb0"},"cell_type":"code","source":"def read_data(which_set):\n    #with open(os.path.join(path_to_project,dataset)+\"\\\\\"+which_set) as file:\n    train=pd.read_csv(which_set)\n    count=0\n    for k in train.keys():\n        count+=1\n    print('number of keys',count)\n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c87d28d412f98786b273dcad84ee511d3f5e0031"},"cell_type":"code","source":"print(os.getcwd())\n#os.chdir('../input')\n#print(os.getcwd())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6740909de93a7f820dc16c64d57e5895ff28639"},"cell_type":"code","source":"training_data=read_data('../input/train.csv')\ntest_data=read_data('../input/test.csv')\nprint(\"sample data points\",training_data.head(10))\nprint(\"data keys\",training_data.keys())\nprint(\"length of each column in training data:\",training_data['qid'].shape)\nprint(\"length of each column in training data:\",test_data['qid'].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"491bef6336d926c281e129fae7df3637901d27a8"},"cell_type":"code","source":"num_data=training_data['qid'].shape[0]\ntraining_data['question_text'][0]\nall_sents=training_data['question_text'].tolist()\nall_sents_test=test_data['question_text'].tolist()\nprint(len(all_sents))\nprint(all_sents[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8375f33d9cc4383d15bb8396994e07f614d7d82"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0845d21f0e284cc6015344ca941002cf5def3d0b"},"cell_type":"code","source":"!pip install nltk\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk import pos_tag\nfrom nltk import NaiveBayesClassifier\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords\n#nltk.download('stopwords')\nglobal stops\nstops=stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60ba5a12f038165895fc0a9c5e836b83b5be6b8a"},"cell_type":"code","source":"print(\"stopwords(english): \",stops)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a5b8106e4d65a488fccc66232c3b0da948c1fa1"},"cell_type":"code","source":"def removestops(text_string):\n    no_stops=[]\n    prepare_word_dict=[]\n    for s in text_string:\n        words=s.split(' ')\n        no_stop_sents=[]\n        for w in words:\n            if not w in stops:\n                w=w.lower()\n                w1=re.sub(pattern='\\W+',string=w,repl='')\n                #if w1 not in prepare_word_dict:\n                prepare_word_dict.append(w1)\n                no_stop_sents.append(w)\n        no_stops.append(no_stop_sents)\n    print(len(prepare_word_dict))\n    print(prepare_word_dict[0:10])\n    return no_stops,prepare_word_dict\nno_stops,prepare_word_dict=removestops(all_sents)\nno_stops_test,prepare_test_dict=removestops(all_sents_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42d9e8561ab5d294569a4f56bb72349dc389ba65"},"cell_type":"code","source":"print(no_stops[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42c78ea2bccf2d6600c362ce16db60970967a87d"},"cell_type":"code","source":"#no_stops_str=[]\n#for sent in no_stops:\n#    no_stops_str.append(' '.join(sent))\n#print(no_stops_str[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb0aef369228881edcd66a500b3b0f7657b7d5f6"},"cell_type":"code","source":"#no_specials=[]\n#for st in no_stops_str:\n#    no_specials.append(re.sub(pattern='\\W+',string=st,repl=' '))\n\n\n\ndef clean_text(s1):\n    s1_str=[]\n    for sent in s1:\n        s1_str.append(' '.join(sent))\n    out_st=[re.sub(pattern='\\W+',string=st,repl=' ') for st in s1_str]\n    return out_st\nno_specials=clean_text(no_stops)\nno_test_specials=clean_text(no_stops_test)\nprint('sentence before: ',all_sents[1],'\\nsentence after: ',no_specials[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1e3540d31671d6533fc258e19295ff3c50e3652"},"cell_type":"markdown","source":"Find out Word embeddings using Word2vec/GloVe"},{"metadata":{"trusted":true,"_uuid":"ae954724e17bc351445e0337c417a5edecfc44bf"},"cell_type":"code","source":"#get vocabulary collection\nprepare_word_dict=list(set(prepare_word_dict))\nprepare_test_dict=list(set(prepare_test_dict))\n'dofs' in prepare_word_dict\nprint(\"number of unique words in train and test data: \",len(prepare_word_dict),' ',len(prepare_test_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fddff6f9a757e49b0f26d914e9b240e6b938bcee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab283788f0b59dedd63d2a6768e4a78bc33aa4dc"},"cell_type":"code","source":"#Prepare training data for word2vec\n#(center word,context)--> learn one-hot repr of context word(input) from one-hpt repr of center word(target) \n#from keras.preprocessing.text import Tokenizer,one_hot\n#onehot_vec=np.zeros(len(prepare_word_dict))\n#word2ind={}\n#ind2word={}\n#for i,word in enumerate(prepare_word_dict):\n#    if word!='':\n#        word2ind[word]=i\n#        ind2word[i]=word\n#use_to_train=[]\n#for st in no_specials:\n#    lst=st.split(' ')\n#    if '' in lst:\n#        lst.remove('')\n#    use_to_train.append(lst)\n#print(use_to_train[1:3])\n#print(len(use_to_train))\n\n\ndef prep_data_for_word_embeds(s1_dict,s2_no_specs):\n    onehot_vec=np.zeros(len(s1_dict))\n    word2ind={}\n    ind2word={}\n    for i,word in enumerate(s1_dict):\n        if word!='':\n            word2ind[word]=i\n            ind2word[i]=word\n    out_data=[]\n    for st in s2_no_specs:\n        lst=st.split(' ')\n        if '' in lst:\n            lst.remove('')\n        out_data.append(lst)\n    \n    return out_data,word2ind,ind2word\n\nuse_to_train,word2ind,ind2word=prep_data_for_word_embeds(prepare_word_dict,no_specials) \nuse_to_test,_,_=prep_data_for_word_embeds(prepare_test_dict,no_test_specials)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9d60a36b4a364659826d822dc495bb9c51391b2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2640148842e25c8f6ec6fbda9e9164d2946716a5"},"cell_type":"code","source":"def get_input_data(in_data):\n    X=[' '.join(ele) for ele in in_data]\n    return X\ntrain_X=get_input_data(use_to_train)\ntest_X=get_input_data(use_to_test)\nprint(\"training data: \",train_X[:5])\nprint(\"test data: \", test_X[:5])\n#emb_model=Word2Vec(sentences=use_to_train,min_count=1,window=window_size,size=50,workers=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1302038db52c7e90e358db309f1433a3437db9bc"},"cell_type":"code","source":"#emb_model.train(sentences=use_to_train,epochs=5,total_examples=len(use_to_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0efe9b193b904aa8a661387f98d5cf6af4b38ff5"},"cell_type":"code","source":"#emb_model.most_similar(positive=['xbox', 'football'], negative=['game'], topn=1)\nfor ele in use_to_train:\n    for i,w in enumerate(ele):\n        if w in word2ind.keys():\n            ele[i]=int(word2ind[w])\nuse_to_train[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5302e9ef5d6201356915bd55f509c103c3683f9b"},"cell_type":"code","source":"lengths_train=[]\nlengths_test=[]\nfor ele in use_to_train:\n    lengths_train.append(len(ele))\nfor ele in use_to_test:\n    lengths_test.append(len(ele))\nmaxlen_train=max(lengths_train)\nmaxlen_test=max(lengths_test)\nmaxlen=max(maxlen_train,maxlen_test)\nmaxfeatures=40000\n#padded=pad_sequences(sequences=use_to_train,maxlen=None,padding='post')\ntokenizer = Tokenizer(num_words=maxfeatures)\ntokenizer.fit_on_texts(list(train_X)+list(test_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\ntest_X=tokenizer.texts_to_sequences(test_X)\ntrain_X = pad_sequences(train_X, maxlen=maxlen)\ntest_X=pad_sequences(test_X,maxlen=maxlen)\n#for ele in use_to_train:\n#    for i in range(len(ele))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa53fd9f98420a8997cc42459d4878466271a305"},"cell_type":"code","source":"print(\"input train shape: \",train_X.shape)\nprint(\"input test shape: \",test_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f664899c2a3193be3ca17ff57f3d1c10950ae8a"},"cell_type":"code","source":"train_y=training_data['target']\nfrom sklearn.model_selection import train_test_split\ntrain1_X,val_X,train1_y,val_y=train_test_split(train_X,train_y,test_size=0.1,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0286c7640f55042ec16bc0f6890486ac23c7f4aa"},"cell_type":"code","source":"from keras.layers import Conv1D,MaxPool2D,LSTM,Input,MaxPool1D,Flatten,Dense,Dropout\nfrom keras.models import Sequential\nfrom keras.utils import multi_gpu_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ce6f7444b5c637a253afcdc6835fce7699d322af"},"cell_type":"code","source":"\ndims=50\nmodel=Sequential()\ninputs=Input(shape=(maxlen,))\nmodel.add(Embedding(maxfeatures,dims))\nmodel.add(Conv1D(kernel_size=11,strides=2,filters=64,padding='same',use_bias=True,activation='relu'))\nmodel.add(MaxPool1D(padding='same',pool_size=3,strides=2))\nmodel.add(Conv1D(kernel_size=6,strides=2,filters=128,activation='relu'))\nmodel.add(MaxPool1D(padding='same',pool_size=2,strides=2))\nmodel.add(Conv1D(kernel_size=3,strides=1,filters=128,padding='same',use_bias=True,activation='relu'))\nmodel.add(MaxPool1D(padding='same',pool_size=2,strides=2))\nmodel.add(LSTM(units=64,dropout=0.9,return_sequences=False))\nmodel.add(Dense(activation='relu',units=64))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Dense(activation='relu',units=10))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Dense(activation='sigmoid',units=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d9756efc36a9b544bba4e0087f8e88c7b1eecfd"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf44a85d21653dff5714f63ce1ef7c858dea9895"},"cell_type":"code","source":"\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.fit(batch_size=256,epochs=10,x=train1_X,y=train1_y,shuffle=True,validation_data=(val_X,val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1698d40189ef15fac5be9666e76464cb0851e94"},"cell_type":"code","source":"def savemodel(model):\n    model_json = model.to_json()\n    with open(os.path.join('..',\"model.json\"), \"w\") as json_file:\n        json_file.write(model_json)\n    # serialize weights to HDF5\n    #model.save_weights(\"model.h5\")\n    print('=====model has been saved in JSON format======')\n    print('=====model has been saved in  format======')\nprint(\"model has been saved in: \",os.getcwd())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25069f8112231954d69c0898d876ef5a08abf862","scrolled":true},"cell_type":"code","source":"#savemodel(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b29e1051720d33bd4669c9960a8176dfd11b575"},"cell_type":"code","source":"predictions=model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73a004b6c1d3fb13087bf21c2aa73627f2ceab7a"},"cell_type":"code","source":"predictions[0:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02c1694efca92a4dddcf63ff24fceb5ae29146d3"},"cell_type":"code","source":"test_probs=np.squeeze(predictions)\npred_classes=np.zeros(np.shape(test_probs))\nfor i in range(test_probs.shape[0]):\n    if test_probs[i]<0.5:\n        pred_classes[i]=0\n    else:\n        pred_classes[i]=1\nprint(pred_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"295705bd9e5419c83f41f7efeeabe58bbb5d4a51"},"cell_type":"code","source":"len(pred_classes[pred_classes==1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c014952733515f6fcc3ed80f2056e8ede24fbfb2"},"cell_type":"code","source":"df=pd.DataFrame(data={'qid':test_data['qid'],'prediction':pred_classes})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da8973956d5887a2b57a91002a760bee32736aba"},"cell_type":"code","source":"df.to_csv(header=True,sep=',',index=False,path_or_buf='input/submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80fd9a0e988325b9e5d0c97d53b49b4c5438bb5c"},"cell_type":"code","source":"os.listdir('..')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13c8b525dba79643f852d97307226991c242cdb7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}